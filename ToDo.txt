1) запихнуть хеши в массив
2) сравнивать на равенство (помечать дубликаты ID шником?)
3) должен работать с ярлыками и другими видами link'ов корректно (не удалять оригиналы, считая что ярплык это его копия)
4) добавить фн-цию мердж
5) мерджинг лог (что было во что слилось) \ выбрать имя папки от куда брать имена

=========
распаралелирвание
обработка аргументов утилиты:
	рекурс\не рекурс
	сравнивать две (и более) папки на дубликаты ( и различия заодно (?) )
		слить [и выделить разность (хз) ]

GUI?

возможность удалять дубликаты так:
{
/1/A.txt
/2/A.txt
/1/2/A.txt

/1/B.txt
/2/B.txt
/1/2/B.txt

оставить один экземпляр дубликатов которые самые страые/новые \ лежат в папке X (выбрать)
}


сделать супер быстрой (через sum by ассеблер? —мотри как он собираеться в асм \ профилируй ...)



=======

ј стоит ли делать если уже есть? —делаю лучше и удобней для себя
https://ru.wikipedia.org/wiki/Fdupes
https://github.com/jbruchon/jdupes

и просто для практики в раст